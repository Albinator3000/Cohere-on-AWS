{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embedding Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the [Amazon Bedrock InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://cohere.com/blog/embedding-archives-wikipedia) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use the 2023 version of [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary) and binary embeddings.\n",
    "\n",
    "### Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. \n",
    "\n",
    "### Int8/byte and binary encoded embeddings\n",
    "Semantic search over large datasets can require a lot of memory because most vector databases store embeddings and vector indices in memory. Dimensionality reduction to conserve memory and reduce costs can perform poorly ([Cohere research](https://arxiv.org/abs/2205.11498?ref=cohere-ai.ghost.io)). \n",
    "\n",
    "Therefore, a better approach is to use a model that uses fewer bits per dimension. Cohere Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Cohere Embed is the first embedding model that natively supports int8/byte and binary embeddings.\n",
    "\n",
    "Binary embeddings give you a 32x reduction in memory and can be searched 40x faster. Given that embeddings are typically stored as float32, an embedding with 1024 dimensions requires 1024 x 4 bytes = 4096 bytes. Using 1 bit per dimension results in a 32x reduction in required memory (or, 4096 * 8 / 1024). See [Cohere int8 & binary embeddings](https://cohere.com/blog/int8-binary-embeddings).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install HF datasets and boto3, the AWS SDK for Python\n",
    "%pip install datasets --quiet\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "### Step 1: Install the Wikipedia embeddings archives published by Cohere\n",
    "\n",
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Import torch, the open-source machine learning library\n",
    "import torch\n",
    "\n",
    "# Load at max 1000 documents and embeddings\n",
    "max_docs = 1000\n",
    "# Use the Simple English Wikipedia subset\n",
    "lang = \"simple\"\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\", lang, split=\"train\", streaming=True)\n",
    "\n",
    "# To verify we have loaded the data, print docs_stream\n",
    "print(docs_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `IterableDataset` object contains a collection of 1000 examples, each with `features` which are the names of the columns for each example.\n",
    "\n",
    "The `emb_int8` is an integer encoded embedding while `emb_ubinary` is a binary encoded embedding for each Wikipedia article article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Create tensor of binary embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create lists of documents and binary embeddings\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc['emb_ubinary'])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "# Convert doc_embeddings into a PyTorch tensor\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://cohere.com/blog/sentence-word-embeddings) of 128 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the tensor shape\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "## Step3: Embed query and compute dot product with document embeddings\n",
    "We can now search these vectors for any query we want. For this toy example, we'll ask a question about Alan Turing since we know the Wikipedia page for Alan Turing is included in this subset of the archive.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product).\n",
    "\n",
    "This shows the top five passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Wikipedia because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [[18, 63, 75, 232, 59, 67, 51, 160, 255, 68, 251, 186, 114, 165, 136, 58, 82, 15, 211, 232, 128, 37, 107, 204, 75, 163, 74, 251, 32, 233, 200, 154, 106, 241, 127, 125, 74, 31, 123, 209, 82, 220, 228, 15, 254, 151, 220, 43, 199, 230, 143, 73, 67, 229, 149, 61, 34, 86, 69, 56, 215, 178, 131, 49, 108, 251, 76, 187, 134, 2, 155, 169, 129, 130, 229, 103, 12, 113, 145, 9, 32, 139, 212, 3, 224, 64, 27, 151, 175, 217, 139, 30, 132, 192, 111, 60, 221, 162, 108, 120, 153, 219, 214, 165, 164, 133, 78, 232, 203, 63, 149, 53, 135, 117, 100, 213, 75, 46, 114, 159, 22, 216, 255, 233, 98, 26, 252, 22]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To use Cohere models on Bedrock we need to install dependencies\n",
    "import boto3, json, logging\n",
    "# Set up the Bedrock client\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create request paramaters for Bedrock\n",
    "model_id = 'cohere.embed-multilingual-v3'\n",
    "accept = '*/*'\n",
    "content_type = 'application/json'\n",
    "embedding_types = [\"ubinary\"]\n",
    "input_type = \"search_query\"\n",
    "\n",
    "# Create the text used for semantic search\n",
    "query = \"Tell me about Alan Turing\"\n",
    "\n",
    "# Set the number of nearest neighbors\n",
    "k = 7\n",
    "\n",
    "body = json.dumps({\n",
    "    \"texts\": [query],\n",
    "    \"input_type\": input_type,\n",
    "    \"embedding_types\": embedding_types}\n",
    ")\n",
    "\n",
    "# Call the Bedrock invoke_model API\n",
    "response = bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=accept,\n",
    "    contentType=content_type\n",
    ")\n",
    "\n",
    "# Load the response into response_body\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "# Extract the binary embeddings\n",
    "query_emb_int8 = response_body['embeddings']['ubinary']\n",
    "print(\"Query embedding:\", query_emb_int8, \"\\n\")\n",
    "\n",
    "# Convert query into a PyTorch tensor\n",
    "query_emb_int8 = torch.tensor(query_emb_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about Alan Turing\n",
      "Alan Turing\n",
      "Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Alan Turing\n",
      "In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Botany\n",
      "Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.\n",
      "https://simple.wikipedia.org/wiki/Botany \n",
      "\n",
      "Creativity\n",
      "Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.\n",
      "https://simple.wikipedia.org/wiki/Creativity \n",
      "\n",
      "Computer\n",
      "In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.\n",
      "https://simple.wikipedia.org/wiki/Computer \n",
      "\n",
      "Alan Turing\n",
      "Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Angel\n",
      "The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "https://simple.wikipedia.org/wiki/Angel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dot score between query embedding and document embeddings\n",
    "dot_scores = torch.mm(query_emb_int8, doc_embeddings.transpose(0, 1))\n",
    "top_k = torch.topk(dot_scores, k)\n",
    "\n",
    "# Print results\n",
    "print(\"Query:\", query)\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(docs[doc_id]['title'])\n",
    "    print(docs[doc_id]['text'])\n",
    "    print(docs[doc_id]['url'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made significant contributions to several fields during his lifetime. He is widely regarded as one of the most influential figures in the development of theoretical computer science and is often credited as being the father of modern computing and artificial intelligence.\n",
      "\n",
      "Turing was born in London, England, in 1912 and showed a talent for science and mathematics from an early age. He attended the University of Cambridge, where he studied mathematics and gained a first-class degree in 1934. He then went on to do postgraduate work at Princeton University, where he received his Ph.D. in mathematics in 1938.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, particularly those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, was instrumental in helping the Allies gain a crucial advantage over the Germans and is believed to have\n"
     ]
    }
   ],
   "source": [
    "# Optionally, send the same query to the Command R model using the Bedrock converse API and compare the output\n",
    "user_message = \"Tell me about Alan Turing.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    response = bedrock_rt.converse(\n",
    "        modelId='cohere.command-r-plus-v1:0',\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
