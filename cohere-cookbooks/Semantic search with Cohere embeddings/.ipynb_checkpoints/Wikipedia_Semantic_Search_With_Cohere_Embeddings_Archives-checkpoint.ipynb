{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embedding Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the Bedrock InvokeModel API to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://txt.cohere.ai/embedding-archives-wikipedia/) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings). \n",
    "\n",
    "### Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install \"cohere<5\" and HF datasets\n",
    "%pip install datasets --quiet\n",
    "# Let's also install boto3\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIlx5RVCP7g7"
   },
   "source": [
    "<!-- Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://txt.cohere.ai/sentence-word-embeddings/) of 768 values. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBa3oxSsP2fv",
    "outputId": "d9d71135-7ac3-4424-d806-2a994a0b456a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "We can now search these vectors for any query we want. For this toy example, we'll ask a question about Wikipedia since we know the Wikipedia page is included in the first 1000 documents we used here.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJGUurziNiYR",
    "outputId": "bb66def9-3d83-46f7-c871-1224eb5714cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the query, then embed it\n",
    "# query = 'Who founded Wikipedia'\n",
    "# response = co.embed(texts=[query], model='multilingual-22-12')\n",
    "# query_embedding = response.embeddings \n",
    "\n",
    "# # print(type(query_embedding))\n",
    "# # print(query_embedding)\n",
    "\n",
    "# query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "\n",
    "# # Compute dot score between query embedding and document embeddings\n",
    "# dot_scores = torch.mm(query_embedding, doc_embeddings.transpose(0, 1))\n",
    "# top_k = torch.topk(dot_scores, k=3)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Query:\", query)\n",
    "# for doc_id in top_k.indices[0].tolist():\n",
    "#     print(docs[doc_id]['title'])\n",
    "#     print(docs[doc_id]['text'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWFroOO2RwMd"
   },
   "source": [
    "This shows the top three passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Wikipedia because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_id = DEFAULT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets import the required modules to run the notebook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3 - Configure the request to the model\n",
    "\n",
    "The developer provides a few things to the model:\n",
    "- A preamble containing instructions about the task and the desired style for the output.\n",
    "- The user request.\n",
    "- A list of tools to the model.\n",
    "- (Optionally) a chat history for the model to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia was founded by Jimmy Wales and Larry Sanger. It was launched on January 15, 2001, as a free, open-content encyclopedia that anyone can edit. Jimmy Wales is often referred to as the \"father of Wikipedia\" and is known for his strong belief in the power of collaborative knowledge sharing. Larry Sanger played a key role in the early development and promotion of Wikipedia, and is considered the co-founder.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Tell me who founded Wikipedia.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    response = bedrock_rt.converse(\n",
    "        modelId=DEFAULT_MODEL,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Cohere Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Enterprises use this model to power search and retrieval systems.import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets numpy faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n",
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load at max 1000 documents + embeddings\n",
    "max_docs = 1000\n",
    "\n",
    "# docs_stream = load_dataset(f\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\", streaming=True)\n",
    "# docs_stream = load_dataset(f\"Cohere/movies\", split=\"train\", streaming=True)\n",
    "\n",
    "lang = \"simple\"\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\", lang, split=\"train\", streaming=True)\n",
    "\n",
    "print(docs_stream)\n",
    "\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "# Printing docs stream\n",
    "print(docs_stream)\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    # doc_embeddings.append(doc['emb'])\n",
    "    # doc_embeddings.append(doc['emb_int8'])\n",
    "    doc_embeddings.append(doc['emb_ubinary'])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "# doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 207, 'score_bin': 322, 'score_cont': 13.732296824455261}, {'doc_id': 285, 'score_bin': 322, 'score_cont': 13.679420351982117}, {'doc_id': 291, 'score_bin': 323, 'score_cont': 13.309430718421936}, {'doc_id': 225, 'score_bin': 327, 'score_cont': 13.071157336235046}, {'doc_id': 888, 'score_bin': 339, 'score_cont': 12.715680480003357}, {'doc_id': 772, 'score_bin': 335, 'score_cont': 12.682390093803406}, {'doc_id': 886, 'score_bin': 333, 'score_cont': 12.474794745445251}]\n",
      "Australia\n",
      "Australia is part of the Commonwealth of Nations. Australia is made up of six states, and two mainland territories. Each state and territory has its own Parliament and makes its own local laws. The Parliament of Australia sits in Canberra and makes laws for the whole country, also known as the Commonwealth or Federation.\n",
      "https://simple.wikipedia.org/wiki/Australia \n",
      "\n",
      "Native American\n",
      "Native Americans are divided into many small nations, called First Nations in Canada and tribes elsewhere.\n",
      "https://simple.wikipedia.org/wiki/Native%20American \n",
      "\n",
      "Native American\n",
      "According to the 2010 United States census, 0.9% of Americans say they are Native American, 2.9 million people, and 0.8% of Americans say they are both Native American and something else.  They are not evenly spread out through the United States.  About a third of the people in Alaska are Native Alaskan and about a sixth of the people in Oklahoma are Native American.\n",
      "https://simple.wikipedia.org/wiki/Native%20American \n",
      "\n",
      "American English\n",
      "American English or US English is the dialect of the English language spoken in the United States of America. It is different in some ways from other types of English, such as British English. Most types of American English came from local dialects in England. During the 18th and 19th centuries, pronunciation changed less in America than in England.\n",
      "https://simple.wikipedia.org/wiki/American%20English \n",
      "\n",
      "Country\n",
      "This can be developed on even further by adding the constituent countries of the United Kingdom, The Kingdom of the Netherlands and the Kingdom of Denmark which could add anywhere from three to eleven morecountries.\n",
      "https://simple.wikipedia.org/wiki/Country \n",
      "\n",
      "Brazil\n",
      "Brazil is divided into 26 states plus the Federal District in five regions (north, south, northeast, southeast and centre-west):\n",
      "https://simple.wikipedia.org/wiki/Brazil \n",
      "\n",
      "Country\n",
      "A country is a distinct territory with defined borders, boundaries, people and government. Most countries are sovereign states while others make up one part of a larger state Every part of the world is in a country. The people that live in a country are referred to as a nation. The government that runs the country is called the state.\n",
      "https://simple.wikipedia.org/wiki/Country \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "doc_embeddings = np.asarray(doc_embeddings, dtype='uint8')\n",
    "#Create the faiss IndexBinaryFlat index\n",
    "num_dim = 1024 \n",
    "index = faiss.IndexBinaryFlat(num_dim)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "def search(index, query, top_k=7):\n",
    "    # Make sure to set input_type=\"search_query\"\n",
    "    # query_emb = co.embed(texts=[query], model=\"embed-multilingual-v3.0\", input_type=\"search_query\", embedding_types=[\"ubinary\", \"float\"]).embeddings\n",
    "    # query_emb_bin = np.asarray(query_emb.ubinary, dtype='uint8')\n",
    "    # query_emb_float = np.asarray(query_emb.float, dtype=\"float32\")\n",
    "    # Function for generating text embeddings\n",
    "    \"\"\"\n",
    "    Generate text embedding by using the Cohere Embed model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The reqest body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "    accept = '*/*'\n",
    "    content_type = 'application/json'\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "    model_id = 'cohere.embed-multilingual-v3'\n",
    "    text1 = \"who was the founder of wikipedia\"\n",
    "    input_type = \"search_query\"\n",
    "    embedding_types = [\"ubinary\", \"float\"]\n",
    "    # Request body\n",
    "    body = json.dumps({\n",
    "        \"texts\": \n",
    "        # [text1],\n",
    "        [query],\n",
    "        \"input_type\": input_type,\n",
    "        \"embedding_types\": embedding_types}\n",
    "    )\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    query_emb_bin = np.asarray(response_body['embeddings']['ubinary'], dtype='uint8')\n",
    "    query_emb_float = np.asarray(response_body['embeddings']['float'], dtype='float32')\n",
    "\n",
    "    # Phase I: Search on the index with a binary\n",
    "    hits_scores, hits_doc_ids = index.search(query_emb_bin, k=min(10*top_k, index.ntotal))\n",
    "\n",
    "    #Get the results in a list of hits\n",
    "    hits = [{'doc_id': doc_id.item(), 'score_bin': score_bin} for doc_id, score_bin in zip(hits_doc_ids[0], hits_scores[0])]\n",
    "\n",
    "    # Phase II: Do a re-scoring with the float query embedding\n",
    "    binary_doc_emb = np.asarray([index.reconstruct(hit['doc_id']) for hit in hits])\n",
    "    binary_doc_emb_unpacked = np.unpackbits(binary_doc_emb, axis=-1).astype(\"int\")\n",
    "    binary_doc_emb_unpacked = 2*binary_doc_emb_unpacked-1\n",
    "\n",
    "    scores_cont = (query_emb_float[0] @ binary_doc_emb_unpacked.T)\n",
    "    for idx in range(len(scores_cont)):\n",
    "        hits[idx]['score_cont'] = scores_cont[idx]\n",
    "\n",
    "    #Sort by largest score_cont\n",
    "    hits.sort(key=lambda x: x['score_cont'], reverse=True)\n",
    "\n",
    "    return hits[0:top_k]\n",
    "\n",
    "# query2 = 'who was the founder of wikipedia'\n",
    "hits = search(index, \"What are the national partks in the united states?\")\n",
    "print(hits)\n",
    "for hit in hits:\n",
    "    doc_id = hit['doc_id']\n",
    "    print(docs[doc_id]['title'])\n",
    "    print(docs[doc_id]['text'])\n",
    "    print(docs[doc_id]['url'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
