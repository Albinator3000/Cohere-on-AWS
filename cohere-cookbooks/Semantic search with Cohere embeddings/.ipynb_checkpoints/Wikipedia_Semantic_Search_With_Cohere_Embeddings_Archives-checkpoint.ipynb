{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embeddings Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the [Amazon Bedrock InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://cohere.com/blog/embedding-archives-wikipedia) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use the 2023 version of [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary) and binary embeddings.\n",
    "\n",
    "### Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. See [What is Semantic Search](https://cohere.com/blog/what-is-semantic-search) to reaad about improvement algorithms such as hierarchical navigable small world (HNSW) and multiple negative ranking loss.\n",
    "\n",
    "### Int8/byte and Binary Encoded Embeddings\n",
    "Semantic search over large datasets can require a lot of memory because most vector databases store embeddings and vector indices in memory. Dimensionality reduction to conserve memory and reduce costs can perform poorly ([Cohere research](https://arxiv.org/abs/2205.11498?ref=cohere-ai.ghost.io)). \n",
    "\n",
    "A better approach is to use a model that uses fewer bits per dimension. Cohere Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Cohere Embed is the first embedding model that natively supports int8/byte and binary embeddings.\n",
    "\n",
    "Binary embeddings give you a 32x reduction in memory and can be searched 40x faster. Given that embeddings are typically stored as float32, an embedding with 1024 dimensions requires 1024 x 4 bytes = 4096 bytes. Using 1 bit per dimension results in a 32x reduction in required memory (or, 4096 * 8 / 1024). See [Cohere int8 & binary embeddings](https://cohere.com/blog/int8-binary-embeddings).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install HF datasets and boto3, the AWS SDK for Python\n",
    "%pip install datasets --quiet\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "### Step 1: Install the Wikipedia embeddings archives published by Cohere\n",
    "\n",
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['_id', 'url', 'title', 'text', 'emb_int8', 'emb_ubinary'],\n",
      "    n_shards: 7\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Import torch, the open-source machine learning library\n",
    "import torch\n",
    "\n",
    "# Load at max 1000 documents and embeddings\n",
    "max_docs = 1000\n",
    "# Use the Simple English Wikipedia subset\n",
    "lang = \"simple\"\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\", lang, split=\"train\", streaming=True)\n",
    "\n",
    "# To verify we have loaded the data, print docs_stream\n",
    "print(docs_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `IterableDataset` object contains a collection of 1000 examples, each with `features` which are the names of the columns for each example.\n",
    "\n",
    "The `emb_int8` is an integer encoded embedding while `emb_ubinary` is a binary encoded embedding for each Wikipedia article article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Create tensor of binary embeddings for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create lists of documents and binary embeddings\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc['emb_ubinary'])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "# Convert doc_embeddings into a PyTorch tensor\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://cohere.com/blog/sentence-word-embeddings) of 128 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the tensor shape\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "### Step3: Embed query and compute dot product with document embeddings\n",
    "We can now search these vectors for any query we want. For this toy example, we'll ask a question about Alan Turing since we know the Wikipedia page for Alan Turing is included in this subset of the archive.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product).\n",
    "\n",
    "This shows the top five passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Wikipedia because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [[18, 63, 75, 232, 59, 67, 51, 160, 255, 68, 251, 186, 114, 165, 136, 58, 82, 15, 211, 232, 128, 37, 107, 204, 75, 163, 74, 251, 32, 233, 200, 154, 106, 241, 127, 125, 74, 31, 123, 209, 82, 220, 228, 15, 254, 151, 220, 43, 199, 230, 143, 73, 67, 229, 149, 61, 34, 86, 69, 56, 215, 178, 131, 49, 108, 251, 76, 187, 134, 2, 155, 169, 129, 130, 229, 103, 12, 113, 145, 9, 32, 139, 212, 3, 224, 64, 27, 151, 175, 217, 139, 30, 132, 192, 111, 60, 221, 162, 108, 120, 153, 219, 214, 165, 164, 133, 78, 232, 203, 63, 149, 53, 135, 117, 100, 213, 75, 46, 114, 159, 22, 216, 255, 233, 98, 26, 252, 22]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To use Cohere models on Bedrock we need to install dependencies\n",
    "import boto3, json, logging\n",
    "# Set up the Bedrock client\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create request paramaters for Bedrock\n",
    "model_id = 'cohere.embed-multilingual-v3'\n",
    "accept = '*/*'\n",
    "content_type = 'application/json'\n",
    "embedding_types = [\"ubinary\"]\n",
    "input_type = \"search_query\"\n",
    "\n",
    "# Create the text used for semantic search\n",
    "query = \"Tell me about Alan Turing\"\n",
    "\n",
    "# Set the number of nearest neighbors\n",
    "k = 7\n",
    "\n",
    "body = json.dumps({\n",
    "    \"texts\": [query],\n",
    "    \"input_type\": input_type,\n",
    "    \"embedding_types\": embedding_types}\n",
    ")\n",
    "\n",
    "# Call the Bedrock InvokeModel API\n",
    "response = bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=accept,\n",
    "    contentType=content_type\n",
    ")\n",
    "\n",
    "# Load the response into response_body\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "# Extract the binary embeddings\n",
    "query_emb_int8 = response_body['embeddings']['ubinary']\n",
    "print(\"Query embedding:\", query_emb_int8, \"\\n\")\n",
    "\n",
    "# Convert query into a PyTorch tensor\n",
    "query_emb_int8 = torch.tensor(query_emb_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's imagine that we didn't know that our documents contain text with information about Alan Turing. The way to search for relevant documents is to search the `doc_embeddings` with the binary embeddings that we just created above. The semantic meaning of our query is captured by the embeddings, and a simliar query will return an embedding with similar elements. A high score for the dot product indicates similarity. See [What is similarity between sentences](https://cohere.com/blog/what-is-similarity-between-sentences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest element from the dot_scores tensor is 2703919\n",
      "As expected, this value is still lower than the dot product of the query embedding and itself 2873374 \n",
      "\n",
      "The query is: Tell me about Alan Turing \n",
      "\n",
      "The below is a list of top k relevant documents:\n",
      "Title: Alan Turing\n",
      "Text: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Botany\n",
      "Text: Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.\n",
      "https://simple.wikipedia.org/wiki/Botany \n",
      "\n",
      "Title: Creativity\n",
      "Text: Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.\n",
      "https://simple.wikipedia.org/wiki/Creativity \n",
      "\n",
      "Title: Computer\n",
      "Text: In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.\n",
      "https://simple.wikipedia.org/wiki/Computer \n",
      "\n",
      "Title: Alan Turing\n",
      "Text: Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\n",
      "https://simple.wikipedia.org/wiki/Alan%20Turing \n",
      "\n",
      "Title: Angel\n",
      "Text: The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "https://simple.wikipedia.org/wiki/Angel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dot score between query embedding and document embeddings\n",
    "dot_scores = torch.mm(query_emb_int8, doc_embeddings.transpose(0, 1))\n",
    "\n",
    "print('The largest element from the dot_scores tensor is', dot_scores.max().item())\n",
    "print('As expected, this value is still lower than the dot product of the query embedding and itself', torch.mm(query_emb_int8, query_emb_int8.transpose(0, 1)).item(), '\\n')\n",
    "\n",
    "# Use topk to return the largest elements of the dot_scores tensor\n",
    "top_k = torch.topk(dot_scores, k)\n",
    "\n",
    "# Print results\n",
    "print(\"The query is:\", query, '\\n\\nThe below is a list of top k relevant documents:')\n",
    "# This loop iterates over the indices of the top k relevant documents\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print('Title:', docs[doc_id]['title'])\n",
    "    print('Text:', docs[doc_id]['text'])\n",
    "    print(docs[doc_id]['url'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to many fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and one of the fathers of computer science and artificial intelligence.\n",
      "\n",
      "Early Life and Education:\n",
      "Alan Mathison Turing was born on June 23, 1912, in Maida Vale, London, United Kingdom. From an early age, he displayed a talent for science and mathematics. He attended Sherborne School, where he excelled in mathematics and science, and later earned a scholarship to study at King's College, University of Cambridge, in 1931. At Cambridge, he studied mathematics, logic, and cryptology, and his interests gradually shifted toward computation and the limits of mechanical computation.\n",
      "\n",
      "Contributions to Computing and Cryptanalysis:\n",
      "1. Turing Machines: One of Turing's most significant contributions to \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made significant contributions to many fields, including computer science, artificial intelligence, cryptography, and mathematics. He is widely considered to be one of the most influential figures in the development of modern computing and one of the key people who helped shape the course of World War II.\n",
      "\n",
      "Turing is perhaps best known for his work at Bletchley Park, the central site for British code-breaking during World War II. He played a crucial role in breaking the German Enigma machine, a seemingly unbreakable encryption device used by the German military to send coded messages. Turing designed a series of techniques and machines, including the \"Turing bombe,\" which helped decipher Enigma messages and provided the Allies with valuable intelligence information. This work is believed to have shortened the war significantly and saved countless lives.\n",
      "\n",
      "After the war, Turing went on to design the Automatic Computing Engine (ACE), one of the first designs for a stored-program computer. \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a British mathematician, computer scientist, and cryptanalyst who made groundbreaking contributions to several fields, particularly in the areas of computing, cryptography, and artificial intelligence. He is widely regarded as one of the most influential figures in the development of modern computing and one of the fathers of computer science and artificial intelligence.\n",
      "\n",
      "Early Life and Education:\n",
      "Alan Mathison Turing was born on June 23, 1912, in Maida Vale, London, United Kingdom. He showed a strong aptitude for science and mathematics from an early age. Turing's early education was interrupted by several moves, but he eventually attended Sherborne School, where he excelled in mathematics and science. In 1931, he gained a scholarship to study at King's College, University of Cambridge, where he graduated with a degree in mathematics in 1934. He subsequently completed a postgraduate degree in cryptography at the Government Code and Cypher School in London.\n",
      "\n",
      "Contributions to Computing and \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optionally, send the same query to the Command R+ model using the Bedrock converse API and compare the output\n",
    "user_message = \"Tell me about Alan Turing.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    for i in range(3):\n",
    "        print('Result of API call', str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId='cohere.command-r-plus-v1:0',\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, '\\n\\n')\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the llm is clearly non-deterministic. [Read more about large language model parameters here](https://cohere.com/blog/llm-parameters-best-outputs-language-ai) to learn about the parameters used to affect model output. However, we can also add the results of the semantic search as context for our prompt to augment the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt is now: Turing was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.Gregor Mendel (1822–1884), Augustinian priest and scientist, and is often called the father of genetics for his study of the inheritance of traits in pea plants.Creativity is the ability of a person or group to make something new and useful or valuable, or the process of making something new and useful or valuable. It happens in all areas of life - science, art, literature and music.In 1837, Charles Babbage proposed the first general mechanical computer, the Analytical Engine. The Analytical Engine contained an Arithmetic Logic Unit, basic flow control, punched cards, and integrated memory. It is the first general-purpose computer concept that could be used for many things and not only one particular computation. However, this computer was never built while Charles Babbage was alive, because he didn't have enough money. In 1910, Henry Babbage, Charles Babbage's youngest son, was able to complete a portion of this machine and perform basic calculations.Using cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.The same cherubim creatures were said to be cast in gold on top of the Ark of the Covenant. Casting metal is one of the oldest forms of artwork, and was attempted by Leonardo da Vinci.\n",
      "Given the information above, answer this question: Tell me about Alan Turing.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty string\n",
    "context = ''\n",
    "# Append the text from the relevant documents to the context\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    context += docs[doc_id]['text']\n",
    "\n",
    "prompt = f\"\"\"{context}\n",
    "Given the information above, answer this question: {user_message}\"\"\"\n",
    "\n",
    "print('The prompt is now:', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of API call 1 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science and is often referred to as the \"father of modern computing.\"\n",
      "\n",
      "As you mentioned, Turing is best known for his concept of the Turing machine, which he introduced in 1936. The Turing machine was a theoretical device that served as a simple model for a general-purpose computer. It could perform any computation if given the appropriate instructions, providing a framework for understanding the capabilities and limitations of computing devices.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, including those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, significantly contributed to the Allied victory. Turing's expertise in mathematics and cryptanalysis proved invaluable in deciphering enemy communications, providing crucial intelligence to the war effort.\n",
      "\n",
      "After the war, Turing continued his work \n",
      "\n",
      "\n",
      "Result of API call 2 :\n",
      "Alan Turing was a pioneering figure in the field of computing and cryptography. He is widely regarded as one of the most influential thinkers of the 20th century for his groundbreaking work in mathematics, computer science, and cryptography during World War II. \n",
      "\n",
      "As mentioned in the provided information, Turing created the theoretical concept of a \"Turing machine\" in 1936, which was a significant contribution to the emerging field of computing. This imaginary machine introduced the idea of a computer program and set the foundation for modern computing. \n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, most notably those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking headquarters, significantly contributed to the Allied victory. After the war, Turing continued his work in computing and also explored other areas of science. \n",
      "\n",
      "However, Turing's life was tragically cut short. Despite his remarkable contributions, he was prosecuted in 195 \n",
      "\n",
      "\n",
      "Result of API call 3 :\n",
      "Alan Turing was a pioneering British mathematician, computer scientist, and cryptanalyst. He is widely regarded as one of the most influential figures in the development of theoretical computer science and is often referred to as the \"father of modern computing.\"\n",
      "\n",
      "As you mentioned, Turing is best known for his concept of the Turing machine, which he introduced in 1936. The Turing machine was a theoretical device that served as a simple model for a general-purpose computer. It could perform any computation if given the appropriate instructions, providing a framework for understanding the capabilities and limitations of computing devices.\n",
      "\n",
      "During World War II, Turing played a crucial role in breaking German military codes, including those generated by the Enigma machine. His work at Bletchley Park, Britain's code-breaking center, significantly contributed to the Allied victory. After the war, Turing continued his work in computer science and artificial intelligence. He designed the Automatic Computing Engine (ACE), one of the first designs for a stored-program \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send the same query to the Command R+ model using the Bedrock converse API but add the results of the semantic search as context\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": prompt}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    for i in range(3):\n",
    "        print('Result of API call', str(i+1), ':')\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = bedrock_rt.converse(\n",
    "            modelId='cohere.command-r-plus-v1:0',\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "        )\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text, '\\n\\n')\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "In this notebook, we discussed the benefits of using different types of embeddings. Dimensionality reduction negatively impacts performance, but Cohere's Embed helps solve this problem by natively supporting int8/byte and binary embeddings. Binary embeddings provide a 32x memory reduction and enable 40x faster search compared to float32 embeddings, offering a highly efficient solution for large-scale semantic search. We used Amazon Bedrock to call Cohere's Embed and Command R+ LLMs. In addition, we can use the results of semantic search as context when sending prompts to a large language model. As seen above, the augmented prompt containing the top k results from the semantic search give much more deterministic responses. This is just one of the techniques we can use to return responses with up to date and domain-specific information. Semantic search also forms the basis for more advanced improvement algorithms such as HNSW and multiple negative ranking loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
