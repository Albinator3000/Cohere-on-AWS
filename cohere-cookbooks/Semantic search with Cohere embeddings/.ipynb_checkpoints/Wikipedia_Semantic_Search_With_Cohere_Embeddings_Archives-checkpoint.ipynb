{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SslnlBaKPeWP"
   },
   "source": [
    "# Wikipedia Semantic Search with Cohere Embedding Archives\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "In this notebook, we demonstrate how to use the Bedrock Converse API to do simple [semantic search](https://txt.cohere.ai/what-is-semantic-search/) on the [Wikipedia embeddings archives](https://txt.cohere.ai/embedding-archives-wikipedia/) published by Cohere. These archives embed Wikipedia sites in multiple languages. In this example, we'll use [Wikipedia Simple English](https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings). \n",
    "\n",
    "## Semantic Search and Text Embeddings\n",
    "Semantic search leverages text embeddings and similarity to find responses based on meaning, not just keywords. Text embeddings represent pieces of text as numeric vectors that encode semantic meaning. These embeddings allow for mathematical comparisons of word and sentence meaning. Multilingual embeddings map text in different languages to the same vector space, enabling semantic search across languages. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "### Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "IUnwp2cYNnP0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's install \"cohere<5\" and HF datasets\n",
    "%pip install cohere --quiet\n",
    "%pip install datasets --quiet\n",
    "# Let's also install boto3\n",
    "%pip install boto3==1.34.120 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZds1apHPsag"
   },
   "source": [
    "Let's now download 1,000 records from the English Wikipedia embeddings archive so we can search it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v8Pogz7gPQwg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import cohere\n",
    "\n",
    "# Add your cohere API key from www.cohere.com\n",
    "co = cohere.Client(\"JxNKCngko24WEtOQ3QpaJhzQgUNdwdD5SGuAXn7i\")  \n",
    "\n",
    "#Load at max 1000 documents + embeddings\n",
    "max_docs = 1000\n",
    "docs_stream = load_dataset(f\"Cohere/wikipedia-22-12-simple-embeddings\", split=\"train\", streaming=True)\n",
    "\n",
    "docs = []\n",
    "doc_embeddings = []\n",
    "\n",
    "for doc in docs_stream:\n",
    "    docs.append(doc)\n",
    "    doc_embeddings.append(doc['emb'])\n",
    "    if len(docs) >= max_docs:\n",
    "        break\n",
    "\n",
    "doc_embeddings = torch.tensor(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIlx5RVCP7g7"
   },
   "source": [
    "Now, `doc_embeddings` holds the embeddings of the first 1,000 documents in the dataset. Each document is represented as an [embeddings vector](https://txt.cohere.ai/sentence-word-embeddings/) of 768 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBa3oxSsP2fv",
    "outputId": "d9d71135-7ac3-4424-d806-2a994a0b456a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbYAXaI4RQiH"
   },
   "source": [
    "We can now search these vectors for any query we want. For this toy example, we'll ask a question about Wikipedia since we know the Wikipedia page is included in the first 1000 documents we used here.\n",
    "\n",
    "To search, we embed the query, then get the nearest neighbors to its embedding (using dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJGUurziNiYR",
    "outputId": "bb66def9-3d83-46f7-c871-1224eb5714cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who founded Wikipedia\n",
      "Wikipedia\n",
      "Larry Sanger and Jimmy Wales are the ones who started Wikipedia. Wales is credited with defining the goals of the project. Sanger created the strategy of using a wiki to reach Wales' goal. On January 10, 2001, Larry Sanger proposed on the Nupedia mailing list to create a wiki as a \"feeder\" project for Nupedia. Wikipedia was launched on January 15, 2001. It was launched as an English-language edition at www.wikipedia.com, and announced by Sanger on the Nupedia mailing list. Wikipedia's policy of \"neutral point-of-view\" was enforced in its initial months, and was similar to Nupedia's earlier \"nonbiased\" policy. Otherwise, there weren't very many rules initially, and Wikipedia operated independently of Nupedia. \n",
      "\n",
      "Wikipedia\n",
      "Wikipedia began as a related project for Nupedia. Nupedia was a free English-language online encyclopedia project. Nupedia's articles were written and owned by Bomis, Inc which was a web portal company. The important people of the company were Jimmy Wales, the person in charge of Bomis, and Larry Sanger, the editor-in-chief of Nupedia. Nupedia was first licensed under the Nupedia Open Content License which was changed to the GNU Free Documentation License before Wikipedia was founded and made their first article when Richard Stallman requested them. \n",
      "\n",
      "Wikipedia\n",
      "Wikipedia was started on January 10, 2001, by Jimmy Wales and Larry Sanger as part of an earlier online encyclopedia named Nupedia. On January 15, 2001, Wikipedia became a separate website of its own. It is a wiki that uses the software MediaWiki (like all other Wikimedia Foundation projects). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the query, then embed it\n",
    "query = 'Who founded Wikipedia'\n",
    "response = co.embed(texts=[query], model='multilingual-22-12')\n",
    "query_embedding = response.embeddings \n",
    "query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "# Compute dot score between query embedding and document embeddings\n",
    "dot_scores = torch.mm(query_embedding, doc_embeddings.transpose(0, 1))\n",
    "top_k = torch.topk(dot_scores, k=3)\n",
    "\n",
    "# Print results\n",
    "print(\"Query:\", query)\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(docs[doc_id]['title'])\n",
    "    print(docs[doc_id]['text'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWFroOO2RwMd"
   },
   "source": [
    "This shows the top three passages that are relevant to the query. We can retrieve more results by changing the `k` value. The question in this simple demo is about Wikipedia because we know that the Wikipedia page is part of the documents in this subset of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_id = DEFAULT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets import the required modules to run the notebook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\", region_name = \"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3 - Configure the request to the model\n",
    "\n",
    "The developer provides a few things to the model:\n",
    "- A preamble containing instructions about the task and the desired style for the output.\n",
    "- The user request.\n",
    "- A list of tools to the model.\n",
    "- (Optionally) a chat history for the model to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia was founded by Jimmy Wales and Larry Sanger.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Who founded Wikipedia.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    response = bedrock_rt.converse(\n",
    "        modelId=DEFAULT_MODEL,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Cohere Embed is a text embedding model that offers leading performance in 100+ languages. It translates text into vector representations which encode semantic meaning. Enterprises use this model to power search and retrieval systems.import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Generating text emdeddings with the Cohere Embed model cohere.embed-english-v3\n",
      "INFO: Successfully generated text with Cohere model cohere.embed-english-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings\n",
      "\tEmbedding 0\n",
      "f l o a t\n",
      "\tEmbedding 1\n",
      "i n t 8\n",
      "Finished generating text embeddings with Cohere model cohere.embed-english-v3.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 77\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished generating text embeddings with Cohere model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Query embedding\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# query_embedding = torch.tensor(query_embedding)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# What I need to work on\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mresponse_body\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Compute dot score between query embedding and document embeddings\u001b[39;00m\n\u001b[1;32m     80\u001b[0m dot_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(response_body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m), doc_embeddings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_id = 'cohere.embed-english-v3'\n",
    "text1 = \"Who founded wikipedia\"\n",
    "input_type = \"search_document\"\n",
    "embedding_types = [\"int8\", \"float\"]\n",
    "\n",
    "\n",
    "# Function for generating text embeddings\n",
    "def generate_text_embeddings(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text embedding by using the Cohere Embed model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The reqest body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        \"Generating text emdeddings with the Cohere Embed model %s\", model_id)\n",
    "\n",
    "    accept = '*/*'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    logger.info(\"Successfully generated text with Cohere model %s\", model_id)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "model_id = 'cohere.embed-english-v3'\n",
    "text1 = \"who founded wikipedia\"\n",
    "input_type = \"search_document\"\n",
    "embedding_types = [\"int8\", \"float\"]\n",
    "\n",
    "\n",
    "body = json.dumps({\n",
    "    \"texts\": \n",
    "    [text1],\n",
    "    \"input_type\": input_type,\n",
    "    \"embedding_types\": embedding_types}\n",
    ")\n",
    "response = generate_text_embeddings(model_id=model_id,body=body)\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "# print(f\"ID: {response_body.get('id')}\")\n",
    "# print(f\"Response type: {response_body.get('response_type')}\")\n",
    "\n",
    "print(\"Embeddings\")\n",
    "for i, embedding in enumerate(response_body.get('embeddings')):\n",
    "    print(f\"\\tEmbedding {i}\")\n",
    "    print(*embedding)\n",
    "\n",
    "# print(\"Texts\")\n",
    "# for i, text in enumerate(response_body.get('texts')):\n",
    "#     print(f\"\\tText {i}: {text}\")\n",
    "\n",
    "print(f\"Finished generating text embeddings with Cohere model {model_id}.\")\n",
    "\n",
    "# Query embedding\n",
    "# query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "# What I need to work on\n",
    "torch.tensor(response_body.get('embeddings')[0])\n",
    "\n",
    "# Compute dot score between query embedding and document embeddings\n",
    "dot_scores = torch.mm(response_body.get('embeddings'), doc_embeddings.transpose(0, 1))\n",
    "top_k = torch.topk(dot_scores, k=3)\n",
    "\n",
    "# Printing embeddings\n",
    "# print(response_body.get('embeddings'))\n",
    "# print(type(response_body.get('embeddings')))\n",
    "# print(response_body.get('embeddings'))\n",
    "\n",
    "# Print results\n",
    "print(\"Query:\", query)\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    print(docs[doc_id]['title'])\n",
    "    print(docs[doc_id]['text'], \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
