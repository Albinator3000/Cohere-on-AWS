{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Guide for Cohere Command R and Command R+ Models on Amazon Bedrock\n",
    "---\n",
    "## Introduction\n",
    "This notebook gives an overview of prompting with Cohere Command R and R+ models on Amazon Bedrock. This notebook references material pre-built and published by Cohere on prompting with their models via Cohere Cookbooks that can be found here: https://docs.cohere.com/page/cookbooks. This notebook will leverage the Amazon Bedrock APIs.\n",
    "\n",
    "Throughout the notebook, we highlight different prompting technqiues such as more familiar methods like few-shot prompting as well as leveraging steps and context to improve the models outputs and response. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock\n",
    "Amazon Bedrock is a fully managed service provided by Amazon Web Services (AWS) that facilitates the use of high-performing foundation models (FMs) from leading AI companies and Amazon itself. It is designed to help developers build and deploy generative AI applications efficiently and securely without the need to manage infrastructure.\n",
    "\n",
    "Leveragin Cohere's models on Amazon Bedrock can supports a wide range of applications. Some of them include:\n",
    "\n",
    "- Text generation (e.g., stories, essays, emails)\n",
    "- Chatbots and virtual assistants\n",
    "- Search and information synthesis\n",
    "- Text summarization\n",
    "\n",
    "\n",
    "Amazon Bedrock simplifies the development and deployment of generative AI applications by providing access to top foundation models, enabling private customization, and offering a serverless, secure, and cost-effective environment for AI innovation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Command R and Command R+ models\n",
    "\n",
    "There are multiple LLMs from Cohere supported on Amazon Bedrock today and these include:\n",
    "1. #### Cohere Command R+\n",
    "- **Description:** Command R+ is Cohere's most powerful generative language model optimized for long-context tasks, such as retrieval-augmented generation (RAG) and multi-step tool use.\n",
    "- **Max Tokens:** 128K\n",
    "- **Languages:** English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese\n",
    "- **Supported Use Cases:** Text generation, text summarization, chat, knowledge assistants, Q&A, RAG.\n",
    "\n",
    "2. #### Cohere Command R\n",
    "- **Description:** Command R is Cohere's generative language model optimized for long-context tasks, such as retrieval-augmented generation (RAG) and tools, and large scale production workloads.\n",
    "- **Max Tokens:** 128K\n",
    "- **Languages:** English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese\n",
    "- **Supported Use Cases:** Text generation, text summarization, chat, knowledge assistants, Q&A, RAG.\n",
    "\n",
    "3. #### Cohere Command\n",
    "- **Description:** Command is Cohere’s generative large language model (LLM).\n",
    "- **Max Tokens:** 4K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Chat, text generation, text summarization.\n",
    "\n",
    "4. #### Cohere Command Light\n",
    "- **Description:** Command Light is a smaller version of Command, Cohere's generative LLM.\n",
    "- **Max Tokens:** 4K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Chat, text generation, text summarization.\n",
    "\n",
    "These are the LLMs available from Cohere on Amazon Bedrock. Also, there are two embeddings models supported on Amazon Bedrock: 1/ Cohere Embed - English and 2/ Cohere Embed - Multilingual.\n",
    "\n",
    "The overall Command R family of models are highly scalable language models that balance high performance with strong accuracy. Command R and Command R+ are a part of the Command R family and are optimized for RAG based workflows such as conversational interaction and long context tasks, enabling companies to move beyond proof of concept and into production. These powerful models are designed to handle complex tasks with high performance and strong accuracy, making them suitable for real-world applications.\n",
    "\n",
    "Command R boasts high precision on RAG and tool use tasks, low latency and high throughput, a long 128,000-token context length, and strong capabilities across 10 key languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.\n",
    "\n",
    "Command R+ is the newest model, optimized for extremely performant conversational interaction and long-context tasks. It is recommended for workflows that lean on complex RAG functionality and multi-step tool use (agents), while Cohere R is well-suited for simpler RAG and single-step tool use tasks, as well as applications where price is a major consideration. The model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "1. Ensure you have requested access to the models provided by Cohere in the Bedrock console by clicking \"model access.\" Instructions can be found here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n",
    "2. Make sure you have the permissions to access Bedrock and you have the correct IAM permissions from your administrator \n",
    "3. Run the following cell to install boto3 and necessary packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To get started using Cohere models on Bedrock we need to install dependencies. Here, we will install all the required dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3 -> 24.1\n",
      "[notice] To update, run: C:\\Users\\warnebre\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 IPython --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets import the required modules to run the notbook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model\n",
    "\n",
    "Below we give choice of using Command R + models and Command R for the rest of our notebook. By default we will select Command R + and we also create the Bedrock client \"bedrock_rt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_id = DEFAULT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets import the required modules to run the notbook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Supported parameters \n",
    "\n",
    "The Cohere Command models have the following inference parameters available today. The one that is required is \"message\" parameter where the others are able to be added in a call to Bedrock where needed:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"message\": string,\n",
    "    \"chat_history\": [\n",
    "        {\n",
    "            \"role\":\"USER or CHATBOT\",\n",
    "            \"message\": string\n",
    "        }\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        {\"title\": string, \"snippet\": string},\n",
    "    ],\n",
    "    \"search_queries_only\" : boolean,\n",
    "    \"preamble\" : string,\n",
    "    \"max_tokens\": int,\n",
    "    \"temperature\": float,\n",
    "    \"p\": float,\n",
    "    \"k\": float,\n",
    "    \"prompt_truncation\" : string,\n",
    "    \"frequency_penalty\" : float,\n",
    "    \"presence_penalty\" : float,\n",
    "    \"seed\" : int,\n",
    "    \"return_prompt\" : boolean,\n",
    "    \"tools\" : [\n",
    "        {\n",
    "            \"name\": string,\n",
    "            \"description\": string,\n",
    "            \"parameter_definitions\": {\n",
    "                \"parameter name\": {\n",
    "                    \"description\": string,\n",
    "                    \"type\": string,\n",
    "                    \"required\": boolean\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"tool_results\" : [\n",
    "        {\n",
    "            \"call\": {\n",
    "                \"name\": string,\n",
    "                \"parameters\": {\n",
    "                \"parameter name\": string\n",
    "                }\n",
    "            },\n",
    "        \"outputs\": [\n",
    "                {\n",
    "                \"text\": string\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"stop_sequences\": [string],\n",
    "    \"raw_prompting\" : boolean\n",
    "\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few inference parameters available for you to use on Bedrock for Cohere Command R model family specifically. In this notebook we wille explore creating an example function for how to call the invoke_model API as well as how to use a few of the above parameters. Subsequent notebooks will dive into examples of using more parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to generate the text\n",
    "def generate_text(prompt, model_id, temp):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\"\n",
    "    }\n",
    "# Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model(\n",
    "        modelId= model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "# Print the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    # Print the response  \n",
    "    return response_body['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Context\n",
    "Providing context when we call Cohere LLMs helps drive a more accurate response. Let's see a few examples of different outputs when we give additional context vs when we don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon reported its fourth-quarter earnings on Thursday, and the company beat expectations on revenue and profit. Here are some key financial highlights from the earnings report:\n",
      "\n",
      "- Revenue: Amazon reported quarterly revenue of $157.3 billion, up 9% from the same period last year and beating expectations of $155.3 billion.\n",
      "- Profit: The company reported a quarterly profit of $3.5 billion, down 10% from the previous year but above expectations of $2.8 billion.\n",
      "- AWS: Amazon Web Services, the company's cloud computing division, continued to be a strong performer. AWS revenue grew 20% year-over-year to $17.8 billion, and operating income increased 29% to $5.2 billion.\n",
      "- North America: Revenue from Amazon's North America business grew 13% year-over-year to $105.4 billion, while operating income decreased 1% to $3.3 billion.\n",
      "- International: Revenue from Amazon's international business grew 3% year-over-year to $44.6 billion, while the division's operating loss widened to $1.9 billion from $919 million in the previous year.\n",
      "- Advertising: Amazon's \"Other\" revenue, which primarily consists of advertising, grew 25% year-over-year to $11.6 billion.\n",
      "- Prime Day: Amazon held its Prime Day shopping event in October during the fourth quarter, which contributed to strong sales during the period. The company said it was the biggest sales event in its history.\n",
      "- Guidance: Amazon provided guidance for the first quarter of 2023, with expected net sales of between $116 billion and $121 billion, representing growth of 3% to 8% compared to the first quarter of 2022. Operating income is expected to be between $1 billion and $4 billion, down from $8.9 billion in the same quarter last year.\n"
     ]
    }
   ],
   "source": [
    "user_input =\"Give financial highlights from Amazon's earnings call\"\n",
    "prompt = user_input\n",
    "response = generate_text(prompt, model_id, .1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the key financial highlights from Amazon's earnings call for the first quarter (Q1) of the year:\n",
      "\n",
      "- Worldwide revenue: $143.3 billion, a 13% increase year-over-year, excluding foreign exchange impact.\n",
      "- Impact of leap year: Added approximately 120 basis points to the year-over-year quarterly revenue growth rate.\n",
      "- Foreign exchange headwind: Unfavorable impact from global currencies weakening against the US dollar, resulting in a $700 million or 50 basis point headwind to revenue.\n",
      "- Worldwide operating income: $15.3 billion, the highest quarterly income ever, and $3.3 billion above guidance.\n",
      "- North America segment revenue: $86.3 billion, a 12% increase year-over-year.\n",
      "- International segment revenue: $31.9 billion, an 11% increase year-over-year, excluding foreign exchange impact.\n",
      "- Third-party seller performance: Revenue from third-party seller services increased 16% year-over-year, excluding foreign exchange impact, with strong unit growth and increased adoption of optional services.\n",
      "- North America segment operating income: $5 billion, an increase of $4.1 billion year-over-year, with a 460 basis point improvement in operating margin.\n",
      "- International segment operating income: $903 million, an improvement of $2.2 billion year-over-year, with a 710 basis point improvement in operating margin, driven by cost efficiencies and volume leverage.\n",
      "- AWS revenue: $25 billion, a 17% increase year-over-year, with a $100 billion annualized revenue run rate. Excluding the leap year impact, AWS revenue increased by approximately 16%.\n",
      "- AWS operating income: $9.4 billion, an increase of $4.3 billion year-over-year, benefiting from the change in the estimated useful life of servers.\n",
      "- Capital investments: Overall capital investments were $48.4 billion in 2023, with plans to significantly increase capital expenditures in 2024 to support AWS growth, including generative AI.\n",
      "\n",
      "These financial highlights demonstrate Amazon's strong performance in Q1, with notable growth in revenue and operating income, particularly in the North America and AWS segments. The company also emphasized the impact of foreign exchange rates and the leap year on its financial results.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Think back to the last time you were a bot helping summarize financial reports for a company to present.\n",
    "\n",
    "Worldwide revenue was $143.3 billion, representing a 13% increase year-over-year, excluding the impact of foreign exchange and near the top end of our guidance range. I'd like to highlight a couple of points to help you interpret our growth rates.\n",
    "\n",
    "First, we saw an impact from leap year in Q1, which added approximately 120 basis points to the year-over-year quarterly revenue growth rate. Second, while I typically talk about growth rates, excluding the impact of year-over-year changes in foreign exchange, we did see an unfavorable impact from global currencies weakening against the US dollar, more than we had planned in Q1. This led to a $700 million or 50 basis point headwind to revenue relative to what we guided. Excluding this FX headwind, we would have exceeded the top end of our guidance range.\n",
    "\n",
    "Worldwide operating income was $15.3 billion, which was our highest quarterly income ever, and it was $3.3 billion above the high end of our guidance range. This was driven by strong operational performance across all three reportable segments and better-than-expected operating leverage, including lower cost to serve. The impact on operating income from our Q1 FX rate headwind was negligible. I'll speak more to our profitability trends in a moment.\n",
    "\n",
    "In the North America segment, first quarter revenue was $86.3 billion, an increase of 12% year-over-year. In the international segment, revenue was $31.9 billion, an increase of 11% year-over-year, excluding the impact of foreign exchange. We remain focused on the inputs that matter most to our customers: selection, price, and convenience.\n",
    "\n",
    "During the quarter, around the world, we help customers save with our shopping events. We added selection, including premium and luxury brands, and we delivered our fastest speeds ever for Prime members. Third-party sellers continue to be an important part of our offering. Third-party seller services revenue increased 16% year-over-year, excluding the impact of foreign exchange. We saw strong 3P unit growth, coupled with increased adoption of our optional services, such as fulfillment and global logistics. For the quarter, third-party seller unit mix was 61%, up 200 basis points year-over-year.\n",
    "\n",
    "Shifting to profitability, North America segment operating income was $5 billion, an increase of $4.1 billion year-over-year. Operating margin was 5.8%, up 460 basis points year-over-year. We saw improvements in our cost to serve, including continued benefit from our work to regionalize our operations, savings from more consolidated customer shipments, and improved leverage driven by strong unit growth and lower transportation rates.\n",
    "\n",
    "In our international segment, operating income was $903 million, an improvement of $2.2 billion year-over-year. Operating margin was 2.8%, up 710 basis points year-over-year. This is primarily driven by our established countries as we improve cost efficiencies through network design enhancements and improved volume leverage. Additionally, we saw good progress in our emerging countries as they expand their customer offerings and make strides on their respective journeys to profitability.\n",
    "\n",
    "Looking ahead, we see several opportunities to further lower cost to serve and improved profitability in our worldwide stores business while still investing to improve the customer experience. Within our fulfillment network, we are focused on investing in our inbound network, streamlining and standardizing process paths, and adding robotics and automation. These improvement opportunities will take time. However, we have a solid plan in place and we like the path we're on.\n",
    "\n",
    "Advertising remains an important contributor to profitability in North America and international segments. We see many opportunities to grow our offerings, both in the areas that are driving growth today like sponsored products and in areas that are newer, like streaming TV ads.\n",
    "\n",
    "Moving to AWS. Revenue was $25 billion, an increase of 17% year-over-year, and AWS is now a $100 billion annualized revenue run rate business. Excluding the impact from leap year, AWS revenue increased approximately 16% year-over-year.\n",
    "\n",
    "During the first quarter, we saw growth in both generative AI and non-generative AI workloads across a diverse group of customers and across industries as companies are shifting their focus towards driving innovation and bringing new workloads to the cloud. Additionally, we continue to see the impact of cost optimizations diminish. While there always be a level of ongoing optimization, we think the majority of the recent cycle is behind us, and we're likely closer to a steady state of these optimization efforts.\n",
    "\n",
    "AWS operating income was $9.4 billion, an increase of $4.3 billion year-over-year. As a reminder, these results include the impact from the change in the estimated useful life of our servers, which primarily benefits the AWS segment. We made progress in managing our infrastructure and fixed costs while still growing at a healthy rate, which has resulted in improved leverage.\n",
    "\n",
    "As we've said in the past, over time, we expect the AWS operating margins to fluctuate, driven in part by the level of investments we are making in the business. We remain focused on driving efficiencies across the business, which enables us to invest to support the strong growth we're seeing in AWS, including generative AI, which brings us to capital investments. As a reminder, we define these as the combination of CapEx plus equipment finance leases.\n",
    "\n",
    "In 2023, overall capital investments were $48.4 billion. As I mentioned, we're seeing strong AWS demand in both generative AI and our non-generative AI workloads, with customers signing up for longer deals, making bigger commitments. Still relatively early days in generative AI and more broadly, the cloud space, and we see sizable opportunity for growth.\n",
    "\n",
    "We anticipate our overall capital expenditures to meaningfully increase year-over-year in 2024, primarily driven by higher infrastructure CapEx to support growth in AWS, including generative AI.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"Give financial highlights from Amazon's earnings call\"\n",
    "\n",
    "prompt = f\"\"\"{context}\n",
    "Given the information above, answer this question: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed a difference in the outputs. This is for a few reasons but the main being that the first example didn't use any context so the model did not know which earnings call to generate highlights for. \n",
    "\n",
    "We did not give the LLM the transcript exerpt that we wanted it to pull from, thus the first example did not give the information we needed. As compared to the second exampled with context, the model is able to understand more of the actual data you'd like to use and this helps provide more accurate answers. The first example did not know which earnings report to pull from, where as the second ask was including specific items from the transcript from the most recent earnings report from Amazon. \n",
    "\n",
    "There are multiple ways to add even more context and also prevent additional hallucinations. In theory we would not want to pass the entire transcript to the LLM. This is because we want to optimize how many tokens we send into the Bedrock API for cost and performance reasons. This also helps the LLM generate more accurate responses when we use prompt strategies and techniques like Retrieval Augment Generation(RAG) that we will dive into next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Techniques\n",
    "Cohere developed core guidance for advanced prompting techniques for their models: https://docs.cohere.com/docs/advanced-prompt-engineering-techniques#few-shot-prompting. The following examples will step through few shot prompting and chain of thought prompting examples for Cohere Command R and Command R+. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting\n",
    "- Few-shot prompting involves providing the model with a small number of examples demonstrating the desired task before asking it to perform that task on new inputs.\n",
    "- Few-shot prompting is useful when you want the model to learn and generalize a specific task or skill from a small number of examples, rather than relying solely on its pre-training.\n",
    "- This technique is also useful for when the task requires understanding a particular format, structure, or pattern that can be effectively conveyed through examples.\n",
    "You need the model's outputs to closely match the style, tone, or conventions demonstrated in the examples.\n",
    "\n",
    "\n",
    "\n",
    "#### Delimiters\n",
    "In the below example you will also see the use of the delimiter '##'. Providing a well-structured and unambiguous prompt can enhance the performance of a large language model (LLM). It is beneficial to place the instructions at the start of the prompt and delineate different sections, such as instructions, context, and resources, using descriptive headers. These headers can be made more prominent by prefixing them with '##'. You can see in the below example that '## Examples' was used to show the start of the examples.\n",
    "\n",
    "\n",
    "Let's see the below example for how you can use few-shot prompting to create outreach emails based on customer sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: We appreciate your feedback\n",
      "\n",
      "Dear [Customer Name],\n",
      "\n",
      "Thank you for sharing your experience with our product. We are glad to hear that you are generally satisfied with its performance. However, we apologize if you feel that the pricing does not align with the features offered.\n",
      "\n",
      "At [Company Name], we strive to provide our customers with the best value for their investment. We understand that cost is an important factor in your decision-making process, and we want to ensure that our products are accessible and offer a competitive advantage.\n",
      "\n",
      "As a token of our appreciation for your feedback, we'd like to offer you an exclusive 10% discount on your next purchase. Please use the code VALUED10 at checkout. Additionally, we'd like to invite you to join our loyalty program, which offers exclusive discounts, early access to new products, and other benefits to help offset the cost of our products.\n",
      "\n",
      "We value your input and would appreciate the opportunity to further discuss your experience. Please feel free to reach out to our customer support team, who will be more than happy to address any concerns or questions you may have.\n",
      "\n",
      "Thank you for choosing [Company Name]. We are committed to continuously improving our products and services to meet your expectations.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "[Company Name]\n"
     ]
    }
   ],
   "source": [
    "examples = \"\"\"\n",
    "## Examples\n",
    "Customer Sentiment: The customer had a negative experience with the product. They found it difficult to use and felt that the instructions were unclear.\n",
    "Marketing Outreach Email:\n",
    "Subject: We're sorry for the inconvenience\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "We're sorry to hear that you had a negative experience with our product. At [Company Name], we strive to provide our customers with high-quality products and exceptional service.\n",
    "\n",
    "We understand that the instructions may have been unclear, and we apologize for any frustration this may have caused. We value your feedback and would like to learn more about your experience to improve our product and documentation.\n",
    "\n",
    "As a token of our appreciation for your patience and understanding, we'd like to offer you a 20% discount on your next purchase. Please use the code FEEDBACK20 at checkout.\n",
    "\n",
    "If you have any further questions or concerns, please don't hesitate to reach out to our customer support team. We're here to help and ensure that you have a positive experience with our products.\n",
    "\n",
    "Thank you for your continued support.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "Customer Sentiment: The customer had a mixed experience with the product. They found some features useful but felt that others were lacking or confusing.\n",
    "Marketing Outreach Email:\n",
    "Subject: We value your feedback\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "Thank you for sharing your experience with our product. We're glad to hear that you found some features useful, but we apologize for any confusion or frustration caused by the lacking or confusing aspects.\n",
    "\n",
    "At [Company Name], we're committed to continuously improving our products and services based on customer feedback. Your input is invaluable in helping us identify areas for improvement and better meet your needs.\n",
    "\n",
    "As a token of our appreciation for your feedback, we'd like to offer you a 15% discount on your next purchase. Please use the code VALUED15 at checkout.\n",
    "\n",
    "We'd also like to invite you to schedule a call with one of our product specialists to discuss your experience in more detail. This will help us better understand your specific needs and concerns, and ensure that future updates address them effectively.\n",
    "\n",
    "Thank you for choosing [Company Name], and we look forward to continuing to serve you in the future.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "Customer Sentiment: The customer was frustrated with the product and felt that it did not meet their expectations. They had issues with the setup process and found the user interface confusing.\n",
    "Marketing Outreach Email:\n",
    "Subject: We're here to help\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "We're sorry to hear that our product did not meet your expectations. At [Company Name], we take customer satisfaction seriously, and we want to ensure that you have a positive experience with our products and services.\n",
    "\n",
    "We understand that the setup process and user interface caused frustration, and we apologize for any inconvenience this may have caused. We value your feedback and would like to learn more about your specific issues to improve our product and documentation.\n",
    "\n",
    "As a token of our appreciation for your patience and understanding, we'd like to offer you a 25% discount on your next purchase. Please use the code SUPPORT25 at checkout.\n",
    "\n",
    "Additionally, we'd like to invite you to schedule a one-on-one session with one of our product experts. During this session, they can walk you through the setup process, address any concerns you may have, and provide personalized guidance to help you get the most out of our product.\n",
    "\n",
    "At [Company Name], we're committed to providing exceptional customer service, and we want to ensure that you have a positive experience moving forward.\n",
    "\n",
    "Thank you for choosing [Company Name], and we look forward to addressing your concerns and exceeding your expectations.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"The customer was generally satisfied with the product but felt that the pricing was too high for the features offered.\"\n",
    "\n",
    "prompt = f\"\"\" ## Below there are a few examples of the creationg of marketing outreach email with an email subject based on a customer's sentiment. Do not include anything additional. Please generate an email based on user input following\n",
    "the below examples {examples} to answer this question: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Steps\n",
    "To steer the model toward generating higher-quality responses, it can be helpful to add instructions for the model to generate intermediate steps before generating the final output. The information generated during these steps helps enrich the model’s context before it generates the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a startup idea for the technology industry:\n",
      "\n",
      "\"AI-powered personal styling platform\"\n",
      "\n",
      "This startup would use artificial intelligence to offer personalized styling services to its customers. The AI technology would analyze the customer's body type, preferences, and existing wardrobe to suggest outfits and recommend new items that would complement their style. The platform could also offer additional services such as virtual try-on, personalized shopping, and styling advice from fashion experts.\n",
      "\n",
      "The target market for this startup would be individuals who want to improve their sense of style but don't have the time or expertise to put together outfits on their own. The AI-powered platform would offer a convenient and personalized way for these individuals to upgrade their wardrobe and feel more confident in their clothing choices.\n",
      "\n",
      "The startup could generate revenue through a subscription model, where customers pay a monthly fee to access the personal styling services. Additionally, the platform could partner with clothing brands and retailers to promote their products to the platform's users, generating additional revenue through affiliate marketing and sponsored content.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"technology\"\n",
    "\n",
    "prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.5)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry: Technology\n",
      "\n",
      "The Problem: Small and medium-sized businesses often lack the financial resources and expertise to invest in robust cybersecurity measures, leaving them vulnerable to cyber attacks. This is a growing concern as cyber threats become more prevalent and sophisticated.\n",
      "\n",
      "Audience: Small and medium-sized businesses (SMBs) that want to protect their digital assets, ensure data security, and mitigate the risk of cyber attacks without breaking the bank.\n",
      "\n",
      "Startup Idea: A cybersecurity-as-a-service platform that offers affordable and comprehensive cybersecurity solutions tailored specifically for SMBs.\n",
      "\n",
      "Solution:\n",
      "- The startup will provide a subscription-based service that offers a suite of cybersecurity tools and services, including threat detection, incident response, vulnerability assessment, and security training for employees.\n",
      "- By leveraging cloud-based technologies and automation, the startup can offer cost-effective solutions that are easy to deploy and manage.\n",
      "- The platform will also provide personalized recommendations and best practices based on the business's specific needs, ensuring that their digital assets are protected without straining their financial resources.\n",
      "- Additionally, the startup will offer 24/7 monitoring and a dedicated security team to respond to any potential threats, giving SMBs peace of mind.\n",
      "\n",
      "Startup Name: \"CyberFortress\"\n",
      "\n",
      "CyberFortress aims to empower small and medium-sized businesses by providing them with enterprise-level cybersecurity protection at a fraction of the cost, helping them stay resilient and secure in the digital landscape.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"technology\"\n",
    "\n",
    "prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\n",
    "First, describe the problem to be solved.\n",
    "Next, describe the target audience of this startup idea.\n",
    "Next, describe the startup idea and how it solves the problem for the target audience.\n",
    "Next, provide a name for the given startup.\n",
    "\n",
    "Use the following format:\n",
    "Industry: <the given industry>\n",
    "The Problem: <the given problem>\n",
    "Audience: <the given target audience>\n",
    "Startup Idea: <the given idea>\n",
    "Startup Name: <the given name>\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### RAG Examples\n",
    "\n",
    " Cohere's Command R family of models targets the emerging “scalable” category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production. Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG). Command R+ is ideal for advanced Retrieval Augmented Generation (RAG) and also provides citation to reduce hallucinations. You may be wondering, What is RAG? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Let's see an example below of this using the Bedrock API when we create a new function for helping us decide which AWS service to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate text leveraging hard coded documents\n",
    "def generate_RAG(prompt, model_id, temp):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\",\n",
    "    'documents': [\n",
    "    {\n",
    "       \"id\": \"s3_page\",\n",
    "       \"title\": \"S3 product page\",\n",
    "       \"snippet\": \"Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance\",\n",
    "       \"url\": \"https://aws.amazon.com/s3/faqs/?nc=sn&loc=7\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ec2_page\",\n",
    "        \"title\": \"EC2 product page\",\n",
    "        \"snippet\": \"Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.\",\n",
    "        \"url\": \"https://aws.amazon.com/ec2/faqs/\",\n",
    "    }, \n",
    "    {\n",
    "        \"id\": \"dynamodb_page\",\n",
    "        \"title\": \"DynamoDB product page\",\n",
    "        \"snippet\": \"Amazon DynamoDB is a fully managed NoSQL database service that makes it easy to build and scale a real-time application.\",\n",
    "        \"url\": \"https://aws.amazon.com/dynamodb/faqs/?refid=9eb02e4d-80e0-4f27-a621-b90b3c870bf3\",\n",
    "    }\n",
    "    ]\n",
    "    }\n",
    "# Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model(\n",
    "        modelId= model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    # Print the response  \n",
    "    responses = []\n",
    "    responses = [response_body['text'], response_body['documents']]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n",
      "\n",
      "Citations[{'id': 's3_page', 'snippet': 'Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance', 'title': 'S3 product page', 'url': 'https://aws.amazon.com/s3/faqs/?nc=sn&loc=7'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What service should I use to store PDFs?\"\n",
    "\n",
    "response = generate_RAG(prompt, model_id, 0.3)\n",
    "\n",
    "#print the response and the citations that are returned from Bedrock API\n",
    "print(response[0], \"\\n\\n\", \"Citations\", response[1], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we only gave a high level description of S3 in the snippet but when we use the \"URL\" key, we are able to give more information. Then, the command R+ model takes care of the RAG approach for us since the model has been trained specifically for these types of use cases. In the Bedrock response, we can also return \"response_body['documents]\" which gives a high level citation for what the model referenced when giving the response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command R+ vs Command R\n",
    "\n",
    "Both of these models are a part of Cohere's newest model family and while they have similar capabilites, there are areas where they differ in terms of performance and responses\n",
    "\n",
    "### Cohere Command R Model\n",
    "\n",
    "**Capabilities:**\n",
    "\n",
    "- Generates coherent and contextually relevant text based on input prompts.\n",
    "- Proficient in understanding and processing natural language, making it suitable for tasks like summarization, translation, and question answering.\n",
    "- Can be fine-tuned with specific datasets to better align with particular use cases or industry-specific language.\n",
    "    Retrieval-Augmented Generation (RAG): Supports RAG, which allows the model to retrieve relevant documents or information from a database to enhance the quality and accuracy of generated responses.\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- Designed to be efficient in generating text with a balance between speed and quality.\n",
    "- Provides accurate and contextually relevant responses for a wide range of natural language processing tasks.\n",
    "- Can be scaled to handle large volumes of requests, making it suitable for enterprise applications.\n",
    "\n",
    "### Cohere Command R+ Model\n",
    "**Capabilities:**\n",
    "\n",
    "- Builds on the capabilities of the Command R model with improved performance in generating high-quality, contextually accurate text.\n",
    "- Offers superior understanding and processing of complex language structures, making it more effective for nuanced tasks.\n",
    "- Provides more advanced fine-tuning options, allowing for deeper customization to meet specific needs.\n",
    "- Capable of handling longer context windows, which is beneficial for tasks requiring extensive background information.\n",
    "- Enhanced retrieval-augmented generation capabilities, allowing for more accurate and contextually relevant information retrieval and integration into responses.\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- Offers improved performance over the Command R model, with higher accuracy and better handling of complex language tasks.\n",
    "- Maintains efficiency while providing faster response times, even with more complex queries.\n",
    "- Enhanced scalability to support even larger volumes of requests and more demanding applications.\n",
    "- More robust in handling diverse and challenging language tasks, providing reliable performance across different use cases.\n",
    "\n",
    "### Observations\n",
    "The Cohere Command R+ model is more performant than the Command R model due to several key enhancements:\n",
    "\n",
    "- **Improved Text Generation:** The Command R+ model is optimized to generate higher-quality text that is more contextually accurate and coherent. For example, we observed Command R+ model provides more specific data points and a slightly more detailed responses with metrics.\n",
    "- **Advanced Language Understanding:** With superior language understanding capabilities, the Command R+ model can process and interpret more complex language structures, leading to more accurate and nuanced responses.\n",
    "- **Extended Context Handling:** The ability to handle longer context windows allows the Command R+ model to incorporate more background information into its responses, improving the relevance and accuracy of the generated text.\n",
    "- **Enhanced RAG Capabilities:** The improved retrieval-augmented generation capabilities of the Command R+ model enable it to retrieve and integrate more accurate and contextually relevant information from external sources, enhancing the overall quality of the responses. We observed that this model can detect which items will clearly answer a user query and are able to make that decision based on the data or documents given.\n",
    "- **Efficiency and Speed:** Despite the increased capabilities, the Command R+ model may have slightly longer response times due to its more complex processing.\n",
    "\n",
    "These enhancements make the Cohere Command R+ model a more powerful and versatile tool for a wide range of natural language processing tasks, providing better performance and more accurate results compared to the Command R model. Let's see an example showing differences in output for the same task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "The following prompts give examples show casing the difference in outputs between Command R and Command R+ when it comes to text generation and RAG capabilities. The responses from Command R lack the detail that Command R+ is able to provide, but it has a lower runtime hence being more efficient. The Command R+ model is able to provide more enhanced details and is aware of other contextual data to enrich the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Command R Model:\n",
      "XYZ Corp's financial health appears robust based on its Q2 2024 report. The company has demonstrated increased revenue, net profit, and cash reserves while successfully reducing its debt-to-equity ratio, indicating improved financial stability. The quarter also saw successful product launches and market expansion, contributing to growth. \n",
      "\n",
      "Potential investors should feel encouraged by XYZ Corp's performance, particularly the improvements in profitability and financial leverage. The company's strategic initiatives for growth, including product diversification and geographic expansion, are paying off. XYZ Corp's strong cash position also enables it to invest in further growth opportunities. \n",
      "\n",
      "The recommendation for potential investors is to consider taking a position in the company, as its fundamentals appear strong, and the future prospects, with continued growth strategies, are promising. However, it's advisable to keep an eye on the company's performance in the coming quarters to ensure the current momentum is sustained.\n",
      "\n",
      "Runtime for Command R Model: 2.4711 seconds\n",
      "\n",
      "\n",
      "\n",
      "Response from Command R+ Model:\n",
      "XYZ Corp's Q2 2024 financial report indicates a strong and improving financial position. The company has seen healthy growth in revenue and net profit, with a 10% and 5% year-over-year increase, respectively. This growth is a result of strong sales performance in key markets and improved operational efficiencies. \n",
      "\n",
      "Additionally, XYZ Corp's financial health is further evidenced by its improved debt-to-equity ratio, indicating reduced financial risk, and a substantial increase in cash reserves, providing flexibility for future investments. The successful launch of new products and market expansion into Asia also bode well for the company's future prospects. \n",
      "\n",
      "For potential investors, XYZ Corp presents an attractive opportunity. The company's solid financial performance, coupled with its strategic initiatives, positions it well for continued growth and market expansion. With a strong balance sheet and positive financial trends, investing in XYZ Corp offers the potential for stable returns and the prospect of benefiting from future growth initiatives. \n",
      "\n",
      "Therefore, based on the positive financial indicators and strategic direction, investing in XYZ Corp could be a prudent decision, offering the potential for capital appreciation and long-term gains.\n",
      "\n",
      "Runtime for Command R+ Model: 6.2903 seconds\n"
     ]
    }
   ],
   "source": [
    "#local function to measure the runtime of each model we will call\n",
    "def measure_runtime(func, prompt, model_id, temp):\n",
    "    start_time = time.time()\n",
    "    result = func(prompt, model_id, temp)\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "    return result, runtime\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "XYZ Corp's Q2 2024 Financial Report:\n",
    "- Revenue: $100 million, representing a 10% increase year-over-year. This growth was primarily driven by strong sales in the North American and European markets.\n",
    "- Net Profit: $15 million, up 5% year-over-year. The increase in net profit was attributed to improved operational efficiencies and cost-cutting measures.\n",
    "- Debt-to-Equity Ratio: 0.8, down from 1.0 last year. The reduction in the debt-to-equity ratio indicates a stronger balance sheet and reduced financial leverage.\n",
    "- Cash Reserves: $30 million, a 20% increase year-over-year. The significant increase in cash reserves provides the company with greater financial flexibility and the ability to invest in future growth opportunities.\n",
    "- New Product Launches: The company successfully launched three new products this quarter, contributing to the overall revenue growth.\n",
    "- Market Expansion: XYZ Corp expanded its market presence in Asia, which is expected to drive future growth.\n",
    "\n",
    "Overall, XYZ Corp has demonstrated solid financial performance this quarter, with notable improvements in revenue, net profit, and cash reserves. \n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "## Context\n",
    "{context}\n",
    "## Question\n",
    "Summarize the company's financial health and provide a brief recommendation for potential investors\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Generate responses and measure runtime\n",
    "response_command_r, runtime_command_r = measure_runtime(generate_text, prompt, COMMAND_R, temp=0.2)\n",
    "# Print the responses side by side\n",
    "print(\"Response from Command R Model:\")\n",
    "print(response_command_r)\n",
    "# Print the runtimes\n",
    "print(f\"\\nRuntime for Command R Model: {runtime_command_r:.4f} seconds\")\n",
    "\n",
    "print(\"\\n\")\n",
    "response_command_r_plus, runtime_command_r_plus = measure_runtime(generate_text, prompt, COMMAND_R_PLUS, temp=0.2)\n",
    "print(\"\\nResponse from Command R+ Model:\")\n",
    "print(response_command_r_plus)\n",
    "print(f\"\\nRuntime for Command R+ Model: {runtime_command_r_plus:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Command R Model:\n",
      "There are number of services available for storing your documents. Some of the popular ones are:\n",
      "- Amazon S3: It is an object storage service that provides industry-leading scalability, data availability, security, and performance.\n",
      "- Amazon DynamoDB: It is a fully managed NoSQL database service which is useful for building and scaling a real-time application.\n",
      "- Amazon EC2: It is a web service that provides resizable compute capacity in the cloud and makes web-scale computing easier for developers.\n",
      "\n",
      "Citations[{'id': 's3_page', 'snippet': 'Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance', 'title': 'S3 product page', 'url': 'https://aws.amazon.com/s3/faqs/?nc=sn&loc=7'}, {'id': 'dynamodb_page', 'snippet': 'Amazon DynamoDB is a fully managed NoSQL database service that makes it easy to build and scale a real-time application.', 'title': 'DynamoDB product page', 'url': 'https://aws.amazon.com/dynamodb/faqs/?refid=9eb02e4d-80e0-4f27-a621-b90b3c870bf3'}, {'id': 'ec2_page', 'snippet': 'Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.', 'title': 'EC2 product page', 'url': 'https://aws.amazon.com/ec2/faqs/'}]\n",
      "\n",
      "Runtime for Command R Model: 5.3355 seconds\n",
      "\n",
      "\n",
      "\n",
      "Response from Command R+ Model:\n",
      "Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance.\n",
      "\n",
      "Citations[{'id': 's3_page', 'snippet': 'Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance', 'title': 'S3 product page', 'url': 'https://aws.amazon.com/s3/faqs/?nc=sn&loc=7'}]\n",
      "\n",
      "Runtime for Command R+ Model: 3.2781 seconds\n"
     ]
    }
   ],
   "source": [
    "#Code to show enhanced RAG using predefined function generate_RAG() used earlier\n",
    "\n",
    "prompt = \"What service should I use to store my documents?\"\n",
    "\n",
    "# Generate responses and measure runtime\n",
    "response_command_r, runtime_command_r = measure_runtime(generate_RAG, prompt, COMMAND_R, temp=0.3)\n",
    "# Print the responses side by side\n",
    "print(\"Response from Command R Model:\")\n",
    "print(response_command_r[0], \"\\n\\n\", \"Citations\", response_command_r[1], sep=\"\")\n",
    "# Print the runtimes\n",
    "print(f\"\\nRuntime for Command R Model: {runtime_command_r:.4f} seconds\")\n",
    "print(\"\\n\")\n",
    "response_command_r_plus, runtime_command_r_plus = measure_runtime(generate_RAG, prompt, COMMAND_R_PLUS, temp=0.2)\n",
    "print(\"\\nResponse from Command R+ Model:\")\n",
    "print(response_command_r_plus[0], \"\\n\\n\", \"Citations\", response_command_r_plus[1], sep=\"\")\n",
    "print(f\"\\nRuntime for Command R+ Model: {runtime_command_r_plus:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we discussed examples of different prompting techniques when using the Cohere Command R and R+ models. Advanced prompting techniques like tool use and conversation history can all be explored to develop even stronger prompts per use case. We explored different base level techniques for Cohere's models that are similar to other general prompting techniques for other LLMs and models from other 3P model providers available on Bedrock today. \n",
    "\n",
    "Finally, we discussed using RAG based approach with Bedrock APIs. It’s worth noting that the prompting techniques discussed, including RAG, does not guarantee accuracy. We explored how providing context to the model is important to inform the model on how to reply but if the documents, data or even prompt information is out of date, the LLM will respond inaccurately. The processes discussed above help provide low lift ways to reduce risk and even with the feature of providing citations helps with the explainability of the model's response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
