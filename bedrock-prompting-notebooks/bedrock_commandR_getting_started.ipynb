{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cohere Command R and Command R+ Models on Amazon Bedrock\n",
    "---\n",
    "#### Introduction\n",
    "In this notebook we will dive into how to get started with Cohere's Command and Command R+ on Bedrock and using the Bedrock invoke_model API specifically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Bedrock\n",
    "Amazon Bedrock is a fully managed service that provides access to a wide range of powerful foundational models(FMs) through an API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohere Command R and Command R+ models\n",
    "Cohere Command R is a family of highly scalable language models that balance high performance with strong accuracy. Command R family – include Command R and Command R+ models – are optimized for RAG based workflows such as conversational interaction and long context tasks, enabling companies to move beyond proof of concept and into production. These powerful models are designed to handle complex tasks with high performance and strong accuracy, making them suitable for real-world applications.\n",
    "\n",
    "Command R boasts high precision on RAG and tool use tasks, low latency and high throughput, a long 128,000-token context length, and strong capabilities across 10 key languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, and Chinese.\n",
    "\n",
    "Command R+ is the newest model, optimized for extremely performant conversational interaction and long-context tasks. It is recommended for workflows that lean on complex RAG functionality and multi-step tool use (agents), while Cohere R is well-suited for simpler RAG and single-step tool use tasks, as well as applications where price is a major consideration. The model is optimized to perform well in the following languages: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Simplified Chinese, and Arabic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites to using Cohere Command R+ and Command models available on Bedrock\n",
    "1. Ensure you have requested access to the models provided by Cohere in the Bedrock console by clicking \"model access.\" Instructions can be found here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n",
    "2. Make sure you have the permissions to access Bedrock and you have the correct IAM permissions from your administrator \n",
    "3. Run the following cell to install boto3 and necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "Here, we will install all the required dependencies to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 IPython --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets import the required modules to run the notbook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Model\n",
    "\n",
    "Below we will set up the use of using Command R + models and Command R for the rest of our notebook. By default we will select Command R + and we also create the bedrock_rt client to use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R_PLUS = \"cohere.command-r-plus-v1:0\"\n",
    "COMMAND_R = \"cohere.command-r-v1:0\"\n",
    "model_id = DEFAULT_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets import the required modules to run the notbook and set up the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_rt = boto3.client(service_name=\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The Cohere Command models have the following inference parameters available today. The one that is required is \"message\" parameter where the others are able to be added in a call to Bedrock where needed:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"message\": string,\n",
    "    \"chat_history\": [\n",
    "        {\n",
    "            \"role\":\"USER or CHATBOT\",\n",
    "            \"message\": string\n",
    "        }\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        {\"title\": string, \"snippet\": string},\n",
    "    ],\n",
    "    \"search_queries_only\" : boolean,\n",
    "    \"preamble\" : string,\n",
    "    \"max_tokens\": int,\n",
    "    \"temperature\": float,\n",
    "    \"p\": float,\n",
    "    \"k\": float,\n",
    "    \"prompt_truncation\" : string,\n",
    "    \"frequency_penalty\" : float,\n",
    "    \"presence_penalty\" : float,\n",
    "    \"seed\" : int,\n",
    "    \"return_prompt\" : boolean,\n",
    "    \"tools\" : [\n",
    "        {\n",
    "            \"name\": string,\n",
    "            \"description\": string,\n",
    "            \"parameter_definitions\": {\n",
    "                \"parameter name\": {\n",
    "                    \"description\": string,\n",
    "                    \"type\": string,\n",
    "                    \"required\": boolean\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"tool_results\" : [\n",
    "        {\n",
    "            \"call\": {\n",
    "                \"name\": string,\n",
    "                \"parameters\": {\n",
    "                \"parameter name\": string\n",
    "                }\n",
    "            },\n",
    "        \"outputs\": [\n",
    "                {\n",
    "                \"text\": string\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"stop_sequences\": [string],\n",
    "    \"raw_prompting\" : boolean\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may notice above, there are quite a few inference parameters available for you to use on Bedrock for Cohere Command R model family specifically. In this notebook we wille explore creating an example function for how to call the invoke_model API as well as how to use a few of the above parameters. Subsequent notebooks will dive into examples of using more parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to generate the text\n",
    "def generate_text(prompt, model_id, temp):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\"\n",
    "    }\n",
    "# Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model(\n",
    "        modelId= model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "# Print the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    #response_body = response_body['text'].replace(\"\\n\", \" \")\n",
    "    # Print the response  \n",
    "    return response_body['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Context\n",
    "Providing context when we call Cohere LLMs helps drive a more accurate response. Let's see a few examples of different outputs when we give additional context vs when we don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input =\"What is a sample itinerary for a trip to Europe\"\n",
    "prompt = user_input\n",
    "response = generate_text(prompt, model_id, .1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Think back to the last time you were traveling around Europe. \n",
    "The trip will lasts 5 days and the first flight leaves seattle and lands in Nice, France at 4 PM on Monday. The flight returns on 3 PM on Friday back to Seattle. \\\n",
    "On the trip you want to go to the top resteraunts in Nice and visit near by areas in Nice for day trips. \\\n",
    "The time of year will be September so you are looking for active activities, eating good food and seeing the best views \"\"\"\n",
    "\n",
    "user_input = \"What is a sample itinerary for a trip in Europe\"\n",
    "\n",
    "prompt = f\"\"\"{context}\n",
    "Given the information above, answer this question: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if you provide more context to the models, they are able to understand more on the formatting or the structured response. There are multiple ways to add even more context for example, to say \"only provide 2 suggestions of activities per day in Europe\" to tailor the response event further.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Techniques\n",
    "Cohere developed core guidance for advanced prompting techniques for their models: https://docs.cohere.com/docs/advanced-prompt-engineering-techniques#few-shot-prompting. The following examples will step through few shot prompting and chain of thought prompting examples for Cohere Command R and Command R+. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting\n",
    "- Few-shot prompting involves providing the model with a small number of examples demonstrating the desired task before asking it to perform that task on new inputs.\n",
    "- Few-shot prompting is useful when you want the model to learn and generalize a specific task or skill from a small number of examples, rather than relying solely on its pre-training.\n",
    "- This technique is also useful for when the task requires understanding a particular format, structure, or pattern that can be effectively conveyed through examples.\n",
    "You need the model's outputs to closely match the style, tone, or conventions demonstrated in the examples.\n",
    "\n",
    "---\n",
    "\n",
    "##### Delimiters\n",
    "In the below example you will also see the use of the delimiter '##'. Providing a well-structured and unambiguous prompt can enhance the performance of a large language model (LLM). It is beneficial to place the instructions at the start of the prompt and delineate different sections, such as instructions, context, and resources, using descriptive headers. These headers can be made more prominent by prefixing them with '##'. You can see in the below example that '## Examples' was used to show the start of the examples.\n",
    "\n",
    "\n",
    "Let's see the below example for how you can use few-shot prompting to create outreach emails based on customer sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "## Examples\n",
    "Customer Sentiment: The customer had a negative experience with the product. They found it difficult to use and felt that the instructions were unclear.\n",
    "Marketing Outreach Email:\n",
    "Subject: We're sorry for the inconvenience\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "We're sorry to hear that you had a negative experience with our product. At [Company Name], we strive to provide our customers with high-quality products and exceptional service.\n",
    "\n",
    "We understand that the instructions may have been unclear, and we apologize for any frustration this may have caused. We value your feedback and would like to learn more about your experience to improve our product and documentation.\n",
    "\n",
    "As a token of our appreciation for your patience and understanding, we'd like to offer you a 20% discount on your next purchase. Please use the code FEEDBACK20 at checkout.\n",
    "\n",
    "If you have any further questions or concerns, please don't hesitate to reach out to our customer support team. We're here to help and ensure that you have a positive experience with our products.\n",
    "\n",
    "Thank you for your continued support.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "Customer Sentiment: The customer had a mixed experience with the product. They found some features useful but felt that others were lacking or confusing.\n",
    "Marketing Outreach Email:\n",
    "Subject: We value your feedback\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "Thank you for sharing your experience with our product. We're glad to hear that you found some features useful, but we apologize for any confusion or frustration caused by the lacking or confusing aspects.\n",
    "\n",
    "At [Company Name], we're committed to continuously improving our products and services based on customer feedback. Your input is invaluable in helping us identify areas for improvement and better meet your needs.\n",
    "\n",
    "As a token of our appreciation for your feedback, we'd like to offer you a 15% discount on your next purchase. Please use the code VALUED15 at checkout.\n",
    "\n",
    "We'd also like to invite you to schedule a call with one of our product specialists to discuss your experience in more detail. This will help us better understand your specific needs and concerns, and ensure that future updates address them effectively.\n",
    "\n",
    "Thank you for choosing [Company Name], and we look forward to continuing to serve you in the future.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "Customer Sentiment: The customer was frustrated with the product and felt that it did not meet their expectations. They had issues with the setup process and found the user interface confusing.\n",
    "Marketing Outreach Email:\n",
    "Subject: We're here to help\n",
    "\n",
    "Dear [Customer Name],\n",
    "\n",
    "We're sorry to hear that our product did not meet your expectations. At [Company Name], we take customer satisfaction seriously, and we want to ensure that you have a positive experience with our products and services.\n",
    "\n",
    "We understand that the setup process and user interface caused frustration, and we apologize for any inconvenience this may have caused. We value your feedback and would like to learn more about your specific issues to improve our product and documentation.\n",
    "\n",
    "As a token of our appreciation for your patience and understanding, we'd like to offer you a 25% discount on your next purchase. Please use the code SUPPORT25 at checkout.\n",
    "\n",
    "Additionally, we'd like to invite you to schedule a one-on-one session with one of our product experts. During this session, they can walk you through the setup process, address any concerns you may have, and provide personalized guidance to help you get the most out of our product.\n",
    "\n",
    "At [Company Name], we're committed to providing exceptional customer service, and we want to ensure that you have a positive experience moving forward.\n",
    "\n",
    "Thank you for choosing [Company Name], and we look forward to addressing your concerns and exceeding your expectations.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "[Company Name]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"The customer was generally satisfied with the product but felt that the pricing was too high for the features offered.\"\n",
    "\n",
    "prompt = f\"\"\" ## Below there are a few examples of the creationg of marketing outreach email with an email subject based on a customer's sentiment. Do not include anything additional. Please generate an email based on user input following\n",
    "the below examples {examples} to answer this question: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Steps\n",
    "To steer the model toward generating higher-quality responses, it can be helpful to add instructions for the model to generate intermediate steps before generating the final output. The information generated during these steps helps enrich the model’s context before it generates the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"technology\"\n",
    "\n",
    "prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.5)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"technology\"\n",
    "\n",
    "prompt = f\"\"\"Generate a startup idea for this industry: {user_input}\n",
    "First, describe the problem to be solved.\n",
    "Next, describe the target audience of this startup idea.\n",
    "Next, describe the startup idea and how it solves the problem for the target audience.\n",
    "Next, provide a name for the given startup.\n",
    "\n",
    "Use the following format:\n",
    "Industry: <the given industry>\n",
    "The Problem: <the given problem>\n",
    "Audience: <the given target audience>\n",
    "Startup Idea: <the given idea>\n",
    "Startup Name: <the given name>\"\"\"\n",
    "\n",
    "response = generate_text(prompt, model_id, temp=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### RAG Examples\n",
    "\n",
    " Command R targets the emerging “scalable” category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production. Command R is a generative model optimized for long context tasks such as retrieval-augmented generation (RAG). Command R+ is ideal for advanced Retrieval Augmented Generation (RAG) and also provides citation to reduce hallucinations. You may be wondering, What is RAG? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Let's see an example below of this using the Bedrock API when we create a new function for helping us decide which AWS service to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to generate the text\n",
    "def generate_RAG(prompt, model_id, temp):\n",
    "    body = {\n",
    "    'message': prompt,\n",
    "    'temperature': temp,\n",
    "    'preamble':\"\",\n",
    "    'documents': [\n",
    "    {\n",
    "       \"id\": \"s3_page\",\n",
    "       \"title\": \"S3 product page\",\n",
    "       \"snippet\": \"Amazon S3 is an object storage service offering industry-leading scalability, data availability, security, and performance\",\n",
    "       \"url\": \"https://aws.amazon.com/s3/faqs/?nc=sn&loc=7\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ec2_page\",\n",
    "        \"title\": \"EC2 product page\",\n",
    "        \"snippet\": \"Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.\",\n",
    "        \"url\": \"https://aws.amazon.com/ec2/faqs/\",\n",
    "    }, \n",
    "    {\n",
    "        \"id\": \"dynamodb_page\",\n",
    "        \"title\": \"DynamoDB product page\",\n",
    "        \"snippet\": \"Amazon DynamoDB is a fully managed NoSQL database service that makes it easy to build and scale a real-time application.\",\n",
    "        \"url\": \"https://aws.amazon.com/dynamodb/faqs/?refid=9eb02e4d-80e0-4f27-a621-b90b3c870bf3\",\n",
    "    }\n",
    "    ]\n",
    "    }\n",
    "# Invoke the Bedrock model\n",
    "    response = bedrock_rt.invoke_model(\n",
    "        modelId= model_id,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "# Print the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    #response_body = response_body['text'].replace(\"\\n\", \" \")\n",
    "    # Print the response  \n",
    "    responses = []\n",
    "    responses = [response_body['text'], response_body['documents']]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What service should I use to store PDFs?\"\n",
    "\n",
    "response = generate_RAG(prompt, model_id, 0.3)\n",
    "\n",
    "#print the response and the citations that are returned from Bedrock API\n",
    "print(response[0], \"\\n\\n\", \"Citations\", response[1], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we only gave a high level description of S3 in the snippet but when we use the \"URL\" key, we are able to give more information. Then, the command R+ model takes care of the RAG approach for us since the model has been trained specifically for these types of use cases. In the Bedrock response, we can also return \"response_body['documents]\" which gives a high level citation for what the model referenced when giving the response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "In this notebook, we discussed examples of different prompting techniques when using the Cohere Command R and R+ models. Advanced prompting techniques like tool use and conversation history can all be explored to develop even stronger prompts per use case. We explored different base level techniques for Cohere's models that are similar to other general prompting techniques for other LLMs and models from other 3P model providers available on Bedrock today. \n",
    "\n",
    "Finally, we discussed using RAG based approach with Bedrock APIs. It’s worth noting that the prompting techniques discussed, including RAG, does not guarantee accuracy. We explored how providing context to the model is important to inform the model on how to reply but if the documents, data or even prompt information is out of date, the LLM will respond inaccurately. The processes discussed above help provide low lift ways to reduce risk and even with the feature of providing citations helps with the explainability of the model's response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
